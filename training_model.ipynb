{"cells":[{"metadata":{"id":"FuR2DI88pW7h","colab_type":"code","outputId":"54ce243d-98b2-427c-b3b4-1491be861f13","colab":{"base_uri":"https://localhost:8080/","height":143},"trusted":true},"cell_type":"code","source":"#input tensorflow and keras\n\nimport keras\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Activation\nfrom keras.layers import Conv2D, MaxPooling2D, Convolution2D\nfrom keras.optimizers import SGD, Adam, RMSprop\nfrom keras.metrics import categorical_accuracy\nfrom keras import regularizers\nfrom PIL import Image\n\nimport numpy as np\nimport pandas as pd\nimport random\nimport tensorflow as tf\nfrom PIL import Image\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nimport os\nprint(os.listdir(\"../input/inceptionresnetv2\"))\n\n# #Online code use for colab\n# from google.colab import drive\n# drive.mount('/content/drive')\n# config = tf.ConfigProto()\n# config.gpu_options.allow_growth = True\n","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"},{"output_type":"stream","text":"['imagenet_class_index.json', 'inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5', 'inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5']\n","name":"stdout"}]},{"metadata":{"id":"AET612S8pW7l","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"#Set seed\nseed = 1990\nrandom.seed(seed)\nnp.random.seed(seed)\ntf.set_random_seed(seed)","execution_count":2,"outputs":[]},{"metadata":{"id":"Io8tamzopW7n","colab_type":"code","outputId":"cec8a0aa-eafe-4db1-89f7-51e355416d1c","colab":{"base_uri":"https://localhost:8080/","height":72},"trusted":true},"cell_type":"code","source":"#append text .png at the end\ndef append_ext(fn):\n    return fn+\".png\"\n\n#convert int to str\ndef convert_int(i):\n    return str(i)\n\n# data preprocessing\nimage_path = \"../input/aptos2019-blindness-detection/train_images\"\ntraindf=pd.read_csv(\"../input/aptos2019-blindness-detection/train.csv\")\ntraindf[\"id_code\"]=traindf[\"id_code\"].apply(append_ext)\ntraindf[\"diagnosis\"]=traindf[\"diagnosis\"].apply(convert_int)\n\n#build the generator\ntrain_datagen = ImageDataGenerator(\n#     rotation_range=10,\n#     width_shift_range=0.2,\n#     height_shift_range=0.2,\n    rescale = 1./255, \n    validation_split=0.8,\n    horizontal_flip=True,\n#     fill_mode='nearest',\n#     zca_whitening=True,\n#     shear_range=0.2,\n#     zoom_range=0.2,\n)","execution_count":3,"outputs":[]},{"metadata":{"id":"LJKTWkrcpW7o","colab_type":"code","outputId":"18313528-8e25-4229-b783-722f0831613b","colab":{"base_uri":"https://localhost:8080/","height":52},"trusted":true},"cell_type":"code","source":"#Set batch size\nbatch_size=32\n\n# flow_from datafreame format\ntrain_data = train_datagen.flow_from_dataframe(\n    dataframe=traindf,\n    directory=image_path,\n    x_col=\"id_code\",\n    y_col=\"diagnosis\",\n    target_size=(300, 300),\n    color_mode='rgb',\n    subset='training',\n    batch_size=batch_size\n)\n\nvalidation_data = train_datagen.flow_from_dataframe(\n    dataframe=traindf,\n    directory=image_path,\n    x_col=\"id_code\",\n    y_col=\"diagnosis\",\n    target_size=(300, 300),\n    color_mode='rgb',\n    subset='validation',\n    batch_size=batch_size\n)\n\n# flow from directory format\n# train_data = train_datagen.flow_from_directory(\n#     directory=image_path,\n#     target_size=(300, 300),\n#     color_mode='rgb',\n#     subset='training',\n#     batch_size=batch_size\n# )\n\n# validation_data = train_datagen.flow_from_directory(\n#     directory=image_path,\n#     target_size=(300, 300),\n#     color_mode='rgb',\n#     subset='validation',\n#     batch_size=batch_size\n# )","execution_count":4,"outputs":[{"output_type":"stream","text":"Found 733 validated image filenames belonging to 5 classes.\nFound 2929 validated image filenames belonging to 5 classes.\n","name":"stdout"}]},{"metadata":{"id":"dHNlV4LVpW7r","colab_type":"code","outputId":"ec54c2e6-99c5-4d64-df19-78983bee0a86","colab":{"base_uri":"https://localhost:8080/","height":836},"trusted":true},"cell_type":"code","source":"#input keras applications\nfrom keras.applications import VGG16\nfrom keras.applications import ResNet50\nfrom keras.applications import NASNetLarge\nfrom keras.applications import InceptionResNetV2\n\n#VGG-16 model\nVGG16_weights_path = '../input/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\nvgg16 = VGG16(include_top=False,input_shape=(300, 300, 3),weights = VGG16_weights_path,classes=5)\n\nmodel = Sequential()\nmodel.add(vgg16)\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu',activity_regularizer=regularizers.l2(0.001)))\nmodel.add(Dense(256, activation='relu',activity_regularizer=regularizers.l2(0.001)))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(5, activation='softmax',activity_regularizer=regularizers.l1(0.001)))\nmodel.summary()\n\n# resNet50 model\n# resnet_weights_path = '../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n# resNet50 = ResNet50(include_top = False, pooling = 'avg', weights = resnet_weights_path,classes=5)\n\n# model = Sequential()\n# model.add(resNet50)\n# model.add(Dense(5, activation='softmax',activity_regularizer=regularizers.l1(0.001)))\n# model.summary()\n\n# NasNetLarge model\n# NASNetLarge_weights_path = '../input/nasnetlarge/NASNet-large-no-top.h5'\n# NasNetLarge = NASNetLarge(include_top = False, pooling = 'avg', weights = NASNetLarge_weights_path,input_shape=(300, 300, 3),classes=5)\n\n# model = Sequential()\n# model.add(NasNetLarge)\n# model.add(Dense(5, activation='softmax',activity_regularizer=regularizers.l1(0.001)))\n# model.summary()\n\n# InceptionResNetV2 model\n# inceptionResNetV2_path = '../input/inceptionresnetv2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5'\n# inceptionResNetV2 = InceptionResNetV2(include_top=False,input_shape=(300, 300, 3),weights = inceptionResNetV2_path,classes=5)\n\n# model = Sequential()\n# model.add(inceptionResNetV2)\n# model.add(Flatten())\n# model.add(Dense(256, activation='relu',activity_regularizer=regularizers.l2(0.001)))\n# model.add(Dense(256, activation='relu',activity_regularizer=regularizers.l2(0.001)))\n# model.add(Dropout(0.25))\n# model.add(Dense(5, activation='softmax',activity_regularizer=regularizers.l1(0.001)))\n# model.summary()\n\n#Set optimizer\nsgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\nrmsprop = RMSprop(lr=1e-4, rho=0.9, epsilon=None, decay=0.0)\nadam = Adam(lr=0.0001)\n\n#model compile\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['categorical_accuracy'])","execution_count":5,"outputs":[{"output_type":"stream","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nvgg16 (Model)                (None, 9, 9, 512)         14714688  \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 41472)             0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 256)               10617088  \n_________________________________________________________________\ndense_2 (Dense)              (None, 256)               65792     \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 256)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 5)                 1285      \n=================================================================\nTotal params: 25,398,853\nTrainable params: 25,398,853\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"id":"2Hqjtf4RqmN0","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"#Call back\nfrom keras.callbacks import TensorBoard\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.callbacks import EarlyStopping\n\ntbCallBack = TensorBoard(log_dir='log',histogram_freq=0,write_graph=True,write_images=False)\ncheckpoint = ModelCheckpoint(filepath='best_aptos_model.hdf5',save_best_only='True',monitor='val_loss')\nearlyStop = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto')\n","execution_count":6,"outputs":[]},{"metadata":{"id":"qoo15mYcpW7u","colab_type":"code","outputId":"92401276-a99e-4a5f-dcd2-ad936e3143c9","colab":{"base_uri":"https://localhost:8080/","height":267},"trusted":true},"cell_type":"code","source":"#Start training....... fit the model\nmodel.fit_generator(\n        train_data,\n        steps_per_epoch=len(train_data),\n        verbose=1, \n        epochs=200,\n        validation_data=validation_data,\n        validation_steps=len(validation_data),\n        callbacks=[tbCallBack,checkpoint,earlyStop] #<-- add earlyStop to get the best model from call back\n) ","execution_count":7,"outputs":[{"output_type":"stream","text":"Epoch 1/200\n23/23 [==============================] - 395s 17s/step - loss: 1.6505 - categorical_accuracy: 0.6033 - val_loss: 1.3978 - val_categorical_accuracy: 0.6784\nEpoch 2/200\n23/23 [==============================] - 345s 15s/step - loss: 1.3399 - categorical_accuracy: 0.6998 - val_loss: 1.3328 - val_categorical_accuracy: 0.6709\nEpoch 3/200\n23/23 [==============================] - 342s 15s/step - loss: 1.2887 - categorical_accuracy: 0.6997 - val_loss: 1.2850 - val_categorical_accuracy: 0.6951\nEpoch 4/200\n23/23 [==============================] - 349s 15s/step - loss: 1.2428 - categorical_accuracy: 0.7006 - val_loss: 1.2706 - val_categorical_accuracy: 0.7132\nEpoch 5/200\n23/23 [==============================] - 346s 15s/step - loss: 1.2206 - categorical_accuracy: 0.7076 - val_loss: 1.2364 - val_categorical_accuracy: 0.6978\nEpoch 6/200\n23/23 [==============================] - 343s 15s/step - loss: 1.1950 - categorical_accuracy: 0.7161 - val_loss: 1.2019 - val_categorical_accuracy: 0.7030\nEpoch 7/200\n23/23 [==============================] - 348s 15s/step - loss: 1.1458 - categorical_accuracy: 0.7236 - val_loss: 1.1633 - val_categorical_accuracy: 0.7060\nEpoch 8/200\n23/23 [==============================] - 343s 15s/step - loss: 1.1144 - categorical_accuracy: 0.7396 - val_loss: 1.1337 - val_categorical_accuracy: 0.7303\nEpoch 9/200\n23/23 [==============================] - 343s 15s/step - loss: 1.0819 - categorical_accuracy: 0.7478 - val_loss: 1.1155 - val_categorical_accuracy: 0.7221\nEpoch 10/200\n23/23 [==============================] - 346s 15s/step - loss: 1.0563 - categorical_accuracy: 0.7451 - val_loss: 1.0752 - val_categorical_accuracy: 0.7364\nEpoch 11/200\n23/23 [==============================] - 345s 15s/step - loss: 1.0444 - categorical_accuracy: 0.7462 - val_loss: 1.0649 - val_categorical_accuracy: 0.7364\nEpoch 12/200\n23/23 [==============================] - 348s 15s/step - loss: 1.0045 - categorical_accuracy: 0.7515 - val_loss: 1.0463 - val_categorical_accuracy: 0.7327\nEpoch 13/200\n23/23 [==============================] - 350s 15s/step - loss: 0.9969 - categorical_accuracy: 0.7558 - val_loss: 1.0548 - val_categorical_accuracy: 0.7224\nEpoch 14/200\n23/23 [==============================] - 349s 15s/step - loss: 0.9720 - categorical_accuracy: 0.7573 - val_loss: 1.0267 - val_categorical_accuracy: 0.7361\nEpoch 15/200\n23/23 [==============================] - 351s 15s/step - loss: 0.9661 - categorical_accuracy: 0.7572 - val_loss: 1.0168 - val_categorical_accuracy: 0.7378\nEpoch 16/200\n23/23 [==============================] - 351s 15s/step - loss: 0.9494 - categorical_accuracy: 0.7540 - val_loss: 0.9970 - val_categorical_accuracy: 0.7371\nEpoch 17/200\n23/23 [==============================] - 342s 15s/step - loss: 0.9293 - categorical_accuracy: 0.7625 - val_loss: 0.9915 - val_categorical_accuracy: 0.7364\nEpoch 18/200\n23/23 [==============================] - 344s 15s/step - loss: 0.9261 - categorical_accuracy: 0.7566 - val_loss: 1.0038 - val_categorical_accuracy: 0.7296\nEpoch 19/200\n23/23 [==============================] - 343s 15s/step - loss: 0.9265 - categorical_accuracy: 0.7572 - val_loss: 0.9891 - val_categorical_accuracy: 0.7405\nEpoch 20/200\n23/23 [==============================] - 341s 15s/step - loss: 0.9098 - categorical_accuracy: 0.7608 - val_loss: 0.9736 - val_categorical_accuracy: 0.7416\nEpoch 21/200\n23/23 [==============================] - 342s 15s/step - loss: 0.8812 - categorical_accuracy: 0.7651 - val_loss: 0.9559 - val_categorical_accuracy: 0.7409\nEpoch 22/200\n23/23 [==============================] - 345s 15s/step - loss: 0.8803 - categorical_accuracy: 0.7644 - val_loss: 0.9546 - val_categorical_accuracy: 0.7364\nEpoch 23/200\n23/23 [==============================] - 343s 15s/step - loss: 0.8535 - categorical_accuracy: 0.7610 - val_loss: 0.9622 - val_categorical_accuracy: 0.7402\nEpoch 24/200\n23/23 [==============================] - 341s 15s/step - loss: 0.8432 - categorical_accuracy: 0.7683 - val_loss: 0.9339 - val_categorical_accuracy: 0.7419\nEpoch 25/200\n23/23 [==============================] - 341s 15s/step - loss: 0.8207 - categorical_accuracy: 0.7708 - val_loss: 0.9421 - val_categorical_accuracy: 0.7412\nEpoch 26/200\n23/23 [==============================] - 348s 15s/step - loss: 0.8161 - categorical_accuracy: 0.7652 - val_loss: 0.9397 - val_categorical_accuracy: 0.7412\nEpoch 27/200\n23/23 [==============================] - 341s 15s/step - loss: 0.8179 - categorical_accuracy: 0.7668 - val_loss: 0.9435 - val_categorical_accuracy: 0.7416\nEpoch 28/200\n23/23 [==============================] - 342s 15s/step - loss: 0.7768 - categorical_accuracy: 0.7724 - val_loss: 0.9393 - val_categorical_accuracy: 0.7443\nEpoch 29/200\n23/23 [==============================] - 341s 15s/step - loss: 0.7587 - categorical_accuracy: 0.7765 - val_loss: 0.9189 - val_categorical_accuracy: 0.7416\nEpoch 30/200\n23/23 [==============================] - 342s 15s/step - loss: 0.7472 - categorical_accuracy: 0.7752 - val_loss: 0.9101 - val_categorical_accuracy: 0.7433\nEpoch 31/200\n23/23 [==============================] - 347s 15s/step - loss: 0.7243 - categorical_accuracy: 0.7871 - val_loss: 0.9215 - val_categorical_accuracy: 0.7419\nEpoch 32/200\n23/23 [==============================] - 352s 15s/step - loss: 0.7096 - categorical_accuracy: 0.7873 - val_loss: 0.9530 - val_categorical_accuracy: 0.7313\nEpoch 33/200\n23/23 [==============================] - 349s 15s/step - loss: 0.6843 - categorical_accuracy: 0.8030 - val_loss: 0.9084 - val_categorical_accuracy: 0.7521\nEpoch 34/200\n23/23 [==============================] - 350s 15s/step - loss: 0.6559 - categorical_accuracy: 0.8421 - val_loss: 0.9120 - val_categorical_accuracy: 0.7545\nEpoch 35/200\n23/23 [==============================] - 350s 15s/step - loss: 0.6445 - categorical_accuracy: 0.8663 - val_loss: 0.9033 - val_categorical_accuracy: 0.7566\nEpoch 36/200\n23/23 [==============================] - 346s 15s/step - loss: 0.6481 - categorical_accuracy: 0.8772 - val_loss: 0.9105 - val_categorical_accuracy: 0.7569\nEpoch 37/200\n23/23 [==============================] - 348s 15s/step - loss: 0.6068 - categorical_accuracy: 0.9034 - val_loss: 0.9146 - val_categorical_accuracy: 0.7593\nEpoch 38/200\n23/23 [==============================] - 350s 15s/step - loss: 0.5795 - categorical_accuracy: 0.9167 - val_loss: 0.9304 - val_categorical_accuracy: 0.7320\nEpoch 39/200\n23/23 [==============================] - 350s 15s/step - loss: 0.5547 - categorical_accuracy: 0.9416 - val_loss: 0.9137 - val_categorical_accuracy: 0.7566\nEpoch 40/200\n23/23 [==============================] - 350s 15s/step - loss: 0.5374 - categorical_accuracy: 0.9440 - val_loss: 0.8927 - val_categorical_accuracy: 0.7504\nEpoch 41/200\n23/23 [==============================] - 350s 15s/step - loss: 0.5080 - categorical_accuracy: 0.9645 - val_loss: 0.9095 - val_categorical_accuracy: 0.7337\nEpoch 42/200\n23/23 [==============================] - 350s 15s/step - loss: 0.4832 - categorical_accuracy: 0.9660 - val_loss: 0.9146 - val_categorical_accuracy: 0.7456\nEpoch 43/200\n23/23 [==============================] - 367s 16s/step - loss: 0.4724 - categorical_accuracy: 0.9739 - val_loss: 0.9343 - val_categorical_accuracy: 0.7320\nEpoch 44/200\n23/23 [==============================] - 369s 16s/step - loss: 0.4601 - categorical_accuracy: 0.9740 - val_loss: 0.9252 - val_categorical_accuracy: 0.7470\nEpoch 45/200\n23/23 [==============================] - 370s 16s/step - loss: 0.4396 - categorical_accuracy: 0.9739 - val_loss: 0.8999 - val_categorical_accuracy: 0.7398\nEpoch 46/200\n23/23 [==============================] - 370s 16s/step - loss: 0.4173 - categorical_accuracy: 0.9769 - val_loss: 0.9137 - val_categorical_accuracy: 0.7593\nEpoch 47/200\n23/23 [==============================] - 372s 16s/step - loss: 0.4032 - categorical_accuracy: 0.9796 - val_loss: 0.9163 - val_categorical_accuracy: 0.7521\nEpoch 48/200\n23/23 [==============================] - 370s 16s/step - loss: 0.3905 - categorical_accuracy: 0.9821 - val_loss: 0.9006 - val_categorical_accuracy: 0.7439\nEpoch 49/200\n23/23 [==============================] - 369s 16s/step - loss: 0.3724 - categorical_accuracy: 0.9891 - val_loss: 0.8781 - val_categorical_accuracy: 0.7538\n","name":"stdout"},{"output_type":"stream","text":"Epoch 50/200\n23/23 [==============================] - 371s 16s/step - loss: 0.3655 - categorical_accuracy: 0.9837 - val_loss: 0.8918 - val_categorical_accuracy: 0.7562\nEpoch 51/200\n23/23 [==============================] - 369s 16s/step - loss: 0.3617 - categorical_accuracy: 0.9878 - val_loss: 0.8896 - val_categorical_accuracy: 0.7436\nEpoch 52/200\n23/23 [==============================] - 373s 16s/step - loss: 0.3542 - categorical_accuracy: 0.9849 - val_loss: 0.8598 - val_categorical_accuracy: 0.7614\nEpoch 53/200\n23/23 [==============================] - 370s 16s/step - loss: 0.3459 - categorical_accuracy: 0.9851 - val_loss: 0.8680 - val_categorical_accuracy: 0.7576\nEpoch 54/200\n23/23 [==============================] - 365s 16s/step - loss: 0.3333 - categorical_accuracy: 0.9878 - val_loss: 0.9052 - val_categorical_accuracy: 0.7446\nEpoch 55/200\n23/23 [==============================] - 358s 16s/step - loss: 0.3292 - categorical_accuracy: 0.9851 - val_loss: 0.8854 - val_categorical_accuracy: 0.7579\nEpoch 56/200\n23/23 [==============================] - 354s 15s/step - loss: 0.3179 - categorical_accuracy: 0.9904 - val_loss: 0.8618 - val_categorical_accuracy: 0.7460\nEpoch 57/200\n23/23 [==============================] - 352s 15s/step - loss: 0.3112 - categorical_accuracy: 0.9905 - val_loss: 0.8632 - val_categorical_accuracy: 0.7566\nEpoch 58/200\n23/23 [==============================] - 351s 15s/step - loss: 0.3050 - categorical_accuracy: 0.9876 - val_loss: 0.8678 - val_categorical_accuracy: 0.7607\nEpoch 59/200\n23/23 [==============================] - 352s 15s/step - loss: 0.3006 - categorical_accuracy: 0.9891 - val_loss: 0.8488 - val_categorical_accuracy: 0.7661\nEpoch 60/200\n23/23 [==============================] - 349s 15s/step - loss: 0.2944 - categorical_accuracy: 0.9904 - val_loss: 0.8611 - val_categorical_accuracy: 0.7545\nEpoch 61/200\n23/23 [==============================] - 350s 15s/step - loss: 0.2913 - categorical_accuracy: 0.9918 - val_loss: 0.8741 - val_categorical_accuracy: 0.7508\nEpoch 62/200\n23/23 [==============================] - 357s 16s/step - loss: 0.2856 - categorical_accuracy: 0.9932 - val_loss: 0.8587 - val_categorical_accuracy: 0.7532\nEpoch 63/200\n23/23 [==============================] - 367s 16s/step - loss: 0.2851 - categorical_accuracy: 0.9878 - val_loss: 0.8665 - val_categorical_accuracy: 0.7528\nEpoch 64/200\n23/23 [==============================] - 373s 16s/step - loss: 0.2809 - categorical_accuracy: 0.9918 - val_loss: 0.8623 - val_categorical_accuracy: 0.7576\nEpoch 65/200\n23/23 [==============================] - 369s 16s/step - loss: 0.2766 - categorical_accuracy: 0.9876 - val_loss: 0.8607 - val_categorical_accuracy: 0.7620\nEpoch 66/200\n23/23 [==============================] - 359s 16s/step - loss: 0.2724 - categorical_accuracy: 0.9918 - val_loss: 0.8543 - val_categorical_accuracy: 0.7583\nEpoch 67/200\n23/23 [==============================] - 357s 16s/step - loss: 0.2713 - categorical_accuracy: 0.9918 - val_loss: 0.8549 - val_categorical_accuracy: 0.7515\nEpoch 68/200\n23/23 [==============================] - 354s 15s/step - loss: 0.2665 - categorical_accuracy: 0.9946 - val_loss: 0.8774 - val_categorical_accuracy: 0.7480\nEpoch 69/200\n23/23 [==============================] - 356s 15s/step - loss: 0.2682 - categorical_accuracy: 0.9917 - val_loss: 0.8538 - val_categorical_accuracy: 0.7446\n","name":"stdout"},{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"<keras.callbacks.History at 0x7fdc5fc93160>"},"metadata":{}}]},{"metadata":{"id":"wgpGTofDpW7w","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"#Evaluate\nmodel.load_weights('best_aptos_model.hdf5')\nloss_and_metrics =model.evaluate_generator(validation_data,len(validation_data))\nprint('Test loss:{}\\nTest accuracy:{}'.format(loss_and_metrics[0], loss_and_metrics[1]))","execution_count":null,"outputs":[]},{"metadata":{"id":"VMWXzcltI7Gg","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"#testing the model\nfrom keras_preprocessing.image import ImageDataGenerator\n\ntestdf=pd.read_csv(\"../input/aptos2019-blindness-detection/sample_submission.csv\")\ntestdf[\"id_code\"]=testdf[\"id_code\"].apply(append_ext)\ntestdf[\"diagnosis\"]=testdf[\"diagnosis\"].apply(convert_int)\nimage_path = \"../input/aptos2019-blindness-detection/test_images\"\n\ntest_datagen=ImageDataGenerator(rescale=1./255.)\ntest_generator = test_datagen.flow_from_dataframe(\n    dataframe=testdf,\n    directory=image_path,\n    x_col=\"id_code\",\n    y_col=\"diagnosis\",\n    target_size=(300, 300),\n    color_mode='rgb',\n    shuffle=False,\n    batch_size=batch_size\n)\n\ntest_generator.reset()\ntest_pred=model.predict_generator(test_generator,steps=len(test_generator),verbose=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(test_pred)\npredicted_class=np.argmax(test_pred,axis=1)\n\nprint(predicted_class)\nimport collections\nprint(collections.Counter(predicted_class))\n\nsample = pd.read_csv(\"../input/aptos2019-blindness-detection/sample_submission.csv\")\nsample.diagnosis = predicted_class.astype(int)\nsample.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"Train_500.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}